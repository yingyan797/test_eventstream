from flask import Flask, Response, stream_with_context, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import json, os
import google.generativeai as genai

app = Flask(__name__)

# Configure CORS properly
CORS(app, resources={
    r"/*": {
        "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type"]
    }
})

# Configure Gemini AI
# Set your API key as environment variable: export GEMINI_API_KEY="your-key-here"
load_dotenv("backend/.env")
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    print("✓ Gemini AI configured successfully")
else:
    print("⚠ Warning: GEMINI_API_KEY not set. Using simulation mode.")
    model = None

@app.route('/stream-ai', methods=['POST'])
def stream_ai():
    """Real Gemini AI streaming endpoint"""
    data = request.get_json()
    user_message = data.get('message', 'Hello, tell me a short story')
    
    def generate_ai():
        if not model:
            # Fallback simulation if API key not set
            yield f"data: {json.dumps({'error': 'GEMINI_API_KEY not configured'})}\n\n"
            return
        
        try:
            generation_config = genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.95,
                top_k=40,
                max_output_tokens=2048,
            )
            
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            ]
            
            # This is the real streaming from Gemini AI
            response_stream = model.generate_content(
                user_message,
                generation_config=generation_config,
                safety_settings=safety_settings,
                stream=True  # KEY: This enables chunk-by-chunk streaming
            )
            
            chunk_count = 0
            # Each chunk arrives as soon as it's generated by the AI
            for chunk in response_stream:
                if chunk.text:
                    data = {
                        'chunk': chunk.text,  # Raw chunk from AI
                        'chunk_id': chunk_count,
                        'type': 'ai_chunk'
                    }
                    yield f"data: {json.dumps(data)}\n\n"
                    chunk_count += 1
            
            # Send completion signal
            yield f"data: {json.dumps({'type': 'complete', 'total_chunks': chunk_count})}\n\n"
            
        except Exception as e:
            error_data = {
                'type': 'error',
                'error': str(e)
            }
            yield f"data: {json.dumps(error_data)}\n\n"
    
    return Response(
        stream_with_context(generate_ai()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )

@app.route('/stream-simulated', methods=['POST'])
def stream_simulated():
    """Simulated streaming with artificial delays for comparison"""
    import time
    data = request.get_json()
    
    def generate_simulated():
        # Simulated response with fixed 1-second delays
        full_response = "This is a simulated response. Each chunk arrives with a fixed 1-second delay. This is different from real AI streaming where chunks arrive as they're generated, which can be irregular and depends on processing complexity."
        
        # Split into words to simulate chunk-by-chunk
        words = full_response.split()
        chunk_size = 5  # words per chunk
        
        for i in range(0, len(words), chunk_size):
            chunk_text = ' '.join(words[i:i+chunk_size]) + ' '
            data = {
                'chunk': chunk_text,
                'chunk_id': i // chunk_size,
                'type': 'simulated_chunk'
            }
            yield f"data: {json.dumps(data)}\n\n"
            time.sleep(1)  # ARTIFICIAL DELAY - this is the key difference
        
        yield f"data: {json.dumps({'type': 'complete'})}\n\n"
    
    return Response(
        stream_with_context(generate_simulated()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )

@app.route('/health')
def health():
    status = {
        'status': 'ok',
        'gemini_configured': model is not None,
        'api_key_set': bool(GEMINI_API_KEY)
    }
    return jsonify(status)

if __name__ == '__main__':
    print("\n" + "="*60)
    print("Gemini AI Streaming Server")
    print("="*60)
    print(f"Backend: http://localhost:5000")
    print(f"Health Check: http://localhost:5000/health")
    
    if GEMINI_API_KEY:
        print(f"✓ Gemini API Key: Configured")
    else:
        print(f"⚠ Gemini API Key: NOT SET")
        print(f"  Set it with: export GEMINI_API_KEY='your-key-here'")
    
    print("="*60 + "\n")
    
    app.run(debug=True, port=5000, threaded=True)